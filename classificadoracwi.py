# -*- coding: utf-8 -*-
"""ClassificadorACWI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aKvEZhXxFSdh_iCN7wwGWV1hfBcT7Vmj

# Importando bibliotecas
"""

#@title
!pip install plotly==4.14.3
!pip install quandl

import pandas as pd
import numpy as np
import pandas_datareader as pdr
import quandl
from ast import literal_eval

from sklearn.model_selection import cross_val_score, cross_validate 
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.tree import plot_tree

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt

# Necessario criar a key no quandl
key_quandl="Mw-vW_dxkPHHfjxjAQsF"

"""# Captura e tratamento dos dados

---

## Dados ACWI
"""

url_acwi = 'https://app2.msci.com/products/service/index/indexmaster/downloadLevelData?output=INDEX_LEVELS&currency_symbol=USD&index_variant=STRD&start_date=19800101&end_date=20210930&data_frequency=DAILY&baseValue=false&index_codes=892400'

# # para captura direto da web:
# df_acwi = pd.read_excel(url_acwi, skiprows=6).dropna()
# df_acwi['Date'] = pd.to_datetime(df_acwi['Date'], errors='coerce', format='%b %d, %Y')
# df_acwi.to_csv('/content/drive/MyDrive/TCC Puc Minas/ACWI.csv', index=False)

# # para executar na maq local
# df_acwi = pd.read_csv('ACWI.csv',parse_dates=['Date'])
  
# para executar no colab
df_acwi = pd.read_csv('/content/drive/MyDrive/TCC Puc Minas/ACWI.csv',parse_dates=['Date'])

df_acwi.rename(columns={df_acwi.columns[1]: df_acwi.columns[1].split()[0].upper()}, inplace=True)
df_acwi['year_week'] = df_acwi['Date'].dt.strftime('%Y-%U')
df_acwi.dropna(inplace=True)
df_acwi = df_acwi[df_acwi['Date']>'1997-01-01'].sort_values('Date')
df_acwi = df_acwi.groupby('year_week').agg('last')
df_acwi.sort_index(inplace=True)
df_acwi['log_diff'] = np.log(df_acwi['ACWI']/df_acwi['ACWI'].shift(1))
df_acwi.dropna(subset=['log_diff'], inplace=True)
df_acwi['std'] = df_acwi['log_diff'].rolling(52).std()
df_acwi.drop(columns='ACWI', inplace=True)
df_acwi

"""## PMI"""

def pmi_us_classified(key=key_quandl):

  df = quandl.get('ISM/MAN_PMI', authtoken=key, start_date="1996-11-01").sort_index()
  df['year_week'] = df.index.strftime('%Y-%U')
  mean_3m = df['PMI'].rolling(3).mean()

  #PMI classification
  df['pmi_us_gt_50_up'] = np.where((df['PMI'] > mean_3m) & (df['PMI'] >=50), 1, 0)
  df['pmi_us_gt_50_down'] = np.where((df['PMI'] < mean_3m) & (df['PMI'] >=50), 1, 0)
  df['pmi_us_lt_50_up'] = np.where((df['PMI'] > mean_3m) & (df['PMI'] < 50), 1, 0)
  df['pmi_us_lt_50_down'] = np.where((df['PMI'] < mean_3m) & (df['PMI'] < 50), 1, 0)

  df.drop(columns=[ 'PMI'], inplace=True)
  return df[df['year_week']>='1997-00'].set_index('year_week')

df_pmi = pmi_us_classified()
df_pmi.head()

"""## Quandl data"""

def get_quandl(id_quandl, curve_diff=None, key=key_quandl):
    df = quandl.get(id_quandl, authtoken=key, start_date="1996-12-01").sort_index()
    if curve_diff:
        if len(curve_diff) == 2:
            col_name = f'{id_quandl.split("/")[1]}: {"-".join(curve_diff)}'
            df[col_name] = df[curve_diff[0]] - df[curve_diff[1]]
            df = df[[col_name]]
        else:
            raise 'Diferença deve ser calculada com 2 pontos'
            
    df['year_week'] = df.index.strftime('%Y-%U')
    df = df.groupby('year_week').agg('last')
    return df

df_quandl = pd.concat([
  get_quandl('USTREASURY/HQMYC', curve_diff=('10.0', '20.0')),
  get_quandl('USTREASURY/YIELD', curve_diff=('3 MO', '7 YR')),
  get_quandl('USTREASURY/YIELD', curve_diff=('10 YR', '20 YR')),
  get_quandl('USTREASURY/YIELD', curve_diff=('10 YR', '30 YR'))
  ], join='outer', axis=1).sort_index()

df_quandl.head()

"""## FRED data"""

series_fred = ['AAA10Y', 'BAMLH0A0HYM2EY', 'CPALTT01USM657N', 'DGS10',
 'DGS3MO', 'DTB1YR', 'DTB3', 'EMVMACROBUS', 'EMVMACROCONSUME', 'EMVMACROFININD',
 'EMVTAXESEMV', 'EPUSOVDEBT', 'FEDFUNDS', 'GEPUCURRENT', 'NCBCMDPMVCE', 'POILBREUSDM',
 'STLFSI2', 'TEDRATE', 'USD3MTD156N', 'VIXCLS', ]

df_fred = pd.concat(
    [pdr.get_data_fred(serie,  start='1996-01-01', end='2021-08-10') for serie in series_fred]
    , join='outer', axis=1).sort_index()

df_fred['year_week'] = df_fred.index.strftime('%Y-%U')
df_fred = df_fred.groupby('year_week').agg('last')

df_fred.head()

"""## Join das capturas"""

df_join = pd.concat([
  df_acwi,
  df_fred,
  df_quandl,
  df_pmi
  ], join='outer', axis=1).sort_index()
df_join.info()

"""# Analise dos dados

---
"""

df = df_join.copy()

"""## ACWI

"""

#@title
def plot_pctchange_std(df, add_plot=[]):
  fig = go.Figure()
  df = df.copy().dropna(subset=['log_diff'])
  fig.add_trace(go.Scatter(    
      x=df['Date'],
      y=df['std'],
      mode='lines',
      showlegend=False,
      legendgroup="std",
      line=dict(width=0.5, color='grey')
      ))
  fig.add_trace(go.Scatter(
      x=df['Date'],
      y=df['std']*-1,
      fill='tonexty',
      name='std',
      legendgroup="std",
      mode='lines', line=dict(width=0.5, color='grey')
      ))
  fig.add_trace(go.Scatter(    
      x=df['Date'],
      y=df['log_diff'],
      mode='lines',
      name='ACWI log diff',
      line=dict(color='royalblue')
      ))

  for col in add_plot:
      fig.add_trace(go.Scatter(
      x=df['Date'],
      y=df[col],
      name=col,
      ))
  fig.update_layout(legend=dict(
    orientation="h",
    yanchor="bottom",
    y=1.02,
    xanchor="right",
    x=1
  ))
  fig.update_layout(title="AWCI semanal")
  fig.update_xaxes(tickformat="%Y-%m-%d")
  fig.update_yaxes(tickformat = '.2%')  
  return fig

plot_pctchange_std(df_acwi).show(renderer='colab')

fig = px.histogram(df, x='log_diff', nbins=150, marginal="box", title='Histrograma ACWI', )
fig.add_vline(x=0, line_dash="dash", opacity=0.7)
fig.show()

neg_returns = df[df['log_diff'] < -0.0]['log_diff']
pd.DataFrame({'ACWI log diff':df['log_diff'].describe(),'| ACWI log diff < 0': neg_returns.describe()})

print(
    'Valores Positivos: {:.2f} %'.format(100*((len(df) - len(neg_returns))/len(df)),2),
    'Valores Negativos: {:.2f} %'.format(100*len(neg_returns)/len(df), 2),
    sep='\n')

df['category'] = np.where(df['log_diff'] < np.quantile(neg_returns,0.5), 1, 0)
df['category'].value_counts()

fig = plot_pctchange_std(df_acwi)
fig.add_trace(go.Scatter(
    x=df[df['category']==1]['Date'],
    y=df[df['category']==1]['category']*-0.018, marker_size=10,
    mode='markers',marker_symbol='triangle-down',
    name='Category = 1',
    marker=dict(color='darkred', opacity=0.6)
    ))

fig.show()

"""## Dataset com todas as variaveis"""

df.drop(columns='std',inplace=True)
df.drop(columns=['category']).describe().T

px.line(df, x='Date', y='YIELD: 10 YR-30 YR')

df.drop(columns='YIELD: 10 YR-30 YR', inplace=True)

"""### Valores Nulos"""

log_diff_null = df['log_diff'].isnull()
df.ffill(inplace=True)
df = df.loc[~log_diff_null.values]
df

"""### Variaveis explicativas numericas"""

numeric_cols= [c for c in df.columns if not c.startswith('pmi')  if c != 'category']
correl = df[numeric_cols].corr()

fig = px.imshow(correl, width=1000, height=1000, color_continuous_scale='RdBu', zmin=-1)
fig.show()

correl[((correl>0.85)&(correl!=1))|(correl<-0.85)].dropna(how='all').dropna(axis=1, how='all').fillna('--')

# df.drop(columns=['USD3MTD156N','DGS3MO', 'FEDFUNDS', 'DTB1YR'], inplace=True)
# Mantido o DTB3 pois são os titulos de curto prazo negociados no mercado secundario

df['DGS10'] = df['DGS10'].pct_change()
df['DTB3'] = df['DTB3'].pct_change()

"""## Variaveis explicativas categoricas

### PMI
"""

df['lag_log_diff'] = df['log_diff'].shift(-1)
df['lag_category'] = df['category'].shift(-1)

desc_list = []
for c in [c for c in df.columns if c.startswith('pmi')]:
  desc = df[(df[c]==1)]['lag_log_diff'].describe()
  desc_list.append(pd.DataFrame(desc.values, columns=[c], index=desc.index))
pd.concat(desc_list, axis=1)

desc_list = []
for c in [c for c in df.columns if c.startswith('pmi')]:
  desc = df[(df[c]==1) & (df['lag_category']==1)]['lag_log_diff'].describe()
  desc_list.append(pd.DataFrame(desc.values, columns=[c], index=desc.index))
pd.concat(desc_list, axis=1)

"""# Preparação dos dados para o Modelo

---
"""

df_model = df_join.copy()
df_model.drop(columns=['USD3MTD156N','DGS3MO', 'FEDFUNDS', 'DTB1YR', 'Date', 'std', 'YIELD: 10 YR-30 YR'], inplace=True)
log_diff_null = df_model['log_diff'].isnull()
df_model.ffill(inplace=True)
df_model = df_model.loc[~log_diff_null.values]

df_model['DGS10'] = df_model['DGS10'].pct_change().replace(np.inf, 0)
df_model['DTB3'] = df_model['DTB3'].pct_change().replace(np.inf, 0)

median_neg = np.quantile(df_model[df_model['log_diff'] < -0.0]['log_diff'],0.5)
df_model['category'] = np.where(df_model['log_diff'] < median_neg, 1, 0)
df_model['category'] = df_model['category'].shift(-1)

df_model.dropna(inplace=True)
df_model

y = df_model['category'].values
X = df_model.drop(columns='category').values

print('Quantidade\n0: {}\n1:  {}'.format(*np.unique(y, return_counts=True)[1]))

"""### Variaveis explicativas com ajuste da escala"""

scaler = MinMaxScaler()
scaler.fit(X)
X = scaler.transform(X)
pd.DataFrame(X).head()

"""# Modelos

---

## Funções e seed para os modelos
"""

class_weight = {1: y[y == 0].size / y.size,
                0: y[y == 1].size / y.size} 
class_weight

SEED = 51
N_ITER =  100   
N_SPLITS = 100
ss = StratifiedShuffleSplit(n_splits=N_SPLITS, test_size=0.2, random_state=SEED)

def valid(model, X, y):
  ss_valid = StratifiedShuffleSplit(n_splits=N_SPLITS, test_size=0.2, random_state=666)
  scores = cross_val_score(model, X, y, scoring='balanced_accuracy', cv=ss_valid, n_jobs=-1)
  results = {
      'Modelo': str(model).split('(')[0],
      'Media': scores.mean(),
      'std': scores.std(),
      'Pior': scores.min(),
      'Melhor': scores.max(),
      'Parametros': model.get_params()
  }
  return pd.DataFrame([results])

  
def hp_tunning(model, params, random_state=SEED, n_iter=N_ITER, cv=ss):
  print(f'Testando hiperparametros para {str(model).split("(")[0]}' )
  clf = RandomizedSearchCV(model, params, random_state=random_state, scoring='balanced_accuracy',
                           n_iter=n_iter, cv=cv, n_jobs=-1, verbose=1)
  rsearch = clf.fit(X, y)
  df_rs = pd.DataFrame(rsearch.cv_results_)
  df_rs = df_rs[[col for col in df_rs.columns if not col.startswith('split')]].sort_values('rank_test_score')
  return rsearch.best_params_, df_rs

"""## **Random forest**"""

rfc = RandomForestClassifier(random_state=SEED, class_weight=class_weight)
rfc.fit(X, y)
fig = plt.figure(figsize=(20, 10))
plot_tree(rfc.estimators_[0], 
          feature_names=df_model.drop(columns='category').columns,
          filled=True,
          rounded=True, );

rfc = RandomForestClassifier(random_state=SEED)
df_default_rfc = valid(rfc, X, y)
df_default_rfc

"""### Ajuste de hiperparametros"""

params_rfc = {
    'n_estimators':range(70, 151, 10),
    'criterion':['gini', 'entropy'], 
    'max_depth':range(20, 41, 5), 
    'min_samples_split':range(15, 25, 2), 
    'min_samples_leaf':range(50, 61, 1), 
    'max_features':['log2', 'auto'],
    'class_weight':[class_weight]
    }

rfc = RandomForestClassifier(random_state=SEED)
best_params_rfc, df_rs_rfc = hp_tunning(rfc, params_rfc)

df_rs_rfc.head()

"""### Melhor Random Forest"""

best_params_rfc

{'class_weight': {0: 0.2170245398773006, 1: 0.7829754601226994},
 'criterion': 'entropy',
 'max_depth': 20,
 'max_features': 'log2',
 'min_samples_leaf': 59,
 'min_samples_split': 19,
 'n_estimators': 80}

rfc = RandomForestClassifier(**best_params_rfc, random_state=SEED)
df_best_rfc = valid(rfc, X, y)
df_best_rfc

rfc.fit(X, y)
fig = plt.figure(figsize=(20, 10))
plot_tree(rfc.estimators_[0], 
          feature_names=df_model.drop(columns='category').columns,
          filled=True,
          rounded=True, );

"""## **SVC**"""

svc = SVC(random_state=SEED)
df_default_svc = valid(svc, X, y)
df_default_svc

svc = SVC(random_state=SEED, class_weight=class_weight)
df_default_svc = valid(svc, X, y)
df_default_svc

"""### Ajuste de hiperparametros"""

params_svc = {
    'C': range(100, 301, 10),
    'kernel':['poly', 'sigmoid', 'rbf', 'linear', ],
    'gamma':['scale', 'auto'],
    'degree': range(1, 10, 1),
    'class_weight':[class_weight]
    }

svc = SVC(random_state=SEED)
best_params_svc, df_rs_svc = hp_tunning(svc, params_svc)

df_rs_svc.head()

"""### Melhor SVC"""

best_params_svc # {'kernel': 'rbf', 'gamma': 'auto', 'degree': 5, 'class_weight': {1: 0.7832436587240584, 0: 0.2167563412759416}, 'C': 160}

df_best_svc = valid(SVC(**best_params_svc), X,y)
df_best_svc

"""## **SGDClassifier**"""

sgd = SGDClassifier(random_state=SEED)
df_default_sgd = valid(sgd, X, y)
df_default_sgd

sgd = SGDClassifier(random_state=SEED, class_weight=class_weight)
df_default_sgd = valid(sgd, X, y)
df_default_sgd

"""### Ajuste de Hiperparametros"""

params_sgd = {
    'loss':['hinge', 'log', 'squared_hinge', 'modified_huber',  'perceptron'],
    'penalty':['l2', 'l1', 'elasticnet'],
    'alpha':[1e-3, 1e-2, 1e-1],
    'max_iter':range(5000, 19001, 2000), 
    'n_iter_no_change': [15],
    'class_weight':[class_weight]
    }

sgd = SGDClassifier(random_state=SEED)
best_params_sgd, df_rs_sgd = hp_tunning(sgd, params_sgd)

df_rs_sgd.head()

"""### Melhor SGDClassifier"""

best_params_sgd # {'penalty': 'elasticnet', 'n_iter_no_change': 15, 'max_iter': 17000, 'loss': 'modified_huber', 'class_weight': {1: 0.7832436587240584, 0: 0.2167563412759416}, 'alpha': 0.01}

best_sgd = SGDClassifier(**best_params_sgd)
df_best_sgd = valid(best_sgd, X, y)
df_best_sgd

"""
# Comparação dos modelos

---"""

try: 
  df_rs_rfc['modelo'] = 'RandomForestClassifier'
  df_rs_svc['modelo'] = 'SVC'
  df_rs_sgd['modelo'] = 'SGDClassifier'
  col = [c for c in df_rs_rfc.columns  if not any([c.startswith('param_'), c.startswith('rank')])]

  df_results = pd.concat([df_rs_rfc[col][:5], 
                          df_rs_svc[col][:5], 
                          df_rs_sgd[col][:5],
                          ]).sort_values('mean_test_score', ascending=False).reset_index()
  df_results.rename(columns={'index': 'n_execucao'}, inplace=True)
  df_results.to_csv('/content/drive/MyDrive/TCC Puc Minas/df_results.csv', index=False)
except:
  df_results = pd.read_csv('/content/drive/MyDrive/TCC Puc Minas/df_results.csv')

df_results[['modelo', 'mean_test_score', 'std_test_score','mean_fit_time' ]].head(15)

fig = px.scatter(df_results, y='mean_test_score', x='mean_fit_time', color='modelo', symbol='modelo', width=800, height=600)
fig.update_traces(marker_size=10)
fig.update_layout(title="Acuracia Balanceada x Tempo de treino", yaxis_range=[0,1], xaxis_range=[0,1])
fig.update_yaxes(tickformat = '.2%')  
fig.show()

"""## Outras metricas"""

scoring = ['accuracy', 'f1', 'neg_log_loss', 'precision', 'recall']

def valid_scores(linha):
  clf = eval(linha[1]+'()')
  if linha[1] == 'SVC':
    clf.set_params(**literal_eval(linha[2]), probability=True)
  else:
    clf.set_params(**literal_eval(linha[2]))
  scores = cross_validate(clf, X, y, scoring=scoring, cv=ss, n_jobs=-1)
  df = pd.DataFrame([{k: v.mean() for k,v in scores.items()}])
  df['modelo'] = linha[1]
  return df[['modelo', 'test_accuracy', 'test_f1','test_neg_log_loss', 
             'test_precision', 'test_recall',]]

df_new_scores = pd.concat([valid_scores(linha) for linha in df_results[['modelo', 'params']].itertuples()])

df_new_scores

"""## Teste pratico"""

qtd = int(y.size * 0.15)
X_train = X[:-qtd]
y_train = y[:-qtd]
X_teste = X[-qtd:]
y_teste = y[-qtd:]

train_class_weight = {1: y_train[y_train == 0].size / y_train.size,
                      0: y_train[y_train == 1].size / y_train.size} 
train_class_weight

df_results['params'].values[0]

params = {'C': 110,
          'class_weight':train_class_weight,
          'degree': 7,
          'gamma': 'auto',
          'kernel': 'rbf'}

svc = SVC(**params, probability=True).fit(X_train, y_train)
predict = svc.predict(X_teste)
proba = svc.predict_proba(X_teste)[:,1]

df_case = df_join.loc[~log_diff_null.values][['Date', 'log_diff']]
df_pred = pd.DataFrame({'real': y_teste, 'predict': predict, 'proba_queda': proba}, index=df_case.index[-qtd:])
df_case = df_case.merge(df_pred, left_index=True, right_index=True, how='outer')
df_case.dropna(inplace=True)

df_case

df_case[df_case['real']==1].sum()

df_case[df_case['predict']==1]

df_case['pred_class'] = df_case['predict'].astype('str')
fig = px.scatter(
    df_case[(df_case['predict']==1)|(df_case['real']==1)],
    y='log_diff',
    x='Date',
    color='pred_class',
)
fig.update_traces(marker_size=8)
fig.update_layout(title="Log Diff e valores previstos")
fig.update_yaxes(tickformat = '.2%')  
fig.show()